/**
 * src/core/modules/Cognition.js
 * å‰é¡è‘‰èªçŸ¥æ¨¡çµ„ (Cognition Module)
 * è² è²¬æ•´åˆæ„ŸçŸ¥ (Emotion)ã€è¨˜æ†¶ (LTM/Persona) èˆ‡æ±ºç­– (LLM)ï¼Œ
 * ä¸¦æ ¹æ“šç•¶å‰æ¨¡å¼ (Angel/Demon/Group) è·¯ç”±å°è©±æˆ–åŸ·è¡Œå·¥å…·ã€‚
 */

import { parentPort } from 'worker_threads'; // ğŸŒŸ æ–°å¢ï¼šå¼•å…¥ parentPort ä¾†ç™¼é€å¿ƒè·³åŒ…
import OpenAI from 'openai';
import { appLogger } from '../../config/logger.js';
import { toolsDeclarations, executeTool } from '../tools/registry.js';
import { groupChatService } from '../services/GroupChatService.js'; 
import { 
    getDemonSystemPrompt, 
    getAngelSystemPrompt,
    getSelfReflectionPrompt,
    getNaturalConversationInstruction,
    getInteractionRulesPrompt
} from '../../config/prompts.js';

// --- å¸¸æ•¸å®šç¾© ---
const MODEL_NAME = 'gemini-2.5-pro';
const MAX_HISTORY_STORE = 60;   // è³‡æ–™åº«ä¿ç•™çš„å°è©±é•·åº¦
const MAX_HISTORY_CONTEXT = 20; // é¤µçµ¦ LLM çš„çŸ­æœŸè¨˜æ†¶é•·åº¦
const MAX_THOUGHT_DEPTH = 999;  // å·¥å…·èª¿ç”¨çš„æœ€å¤§éè¿´æ·±åº¦

export class CognitionModule {
    /**
     * @param {Object} repo - è³‡æ–™å€‰å„²
     * @param {Object} emotion - æƒ…ç·’æ¨¡çµ„
     * @param {Object} persona - äººæ ¼èˆ‡è¨˜æ†¶æ¨¡çµ„
     * @param {Object} ltm - é•·æœŸæƒ…ç¯€è¨˜æ†¶æ¨¡çµ„
     */
    constructor(repo, emotion, persona, ltm) {
        if (!repo || !emotion || !persona || !ltm) {
            throw new Error('[Cognition] Missing required dependencies.');
        }

        this.repo = repo;
        this.emotion = emotion; 
        this.persona = persona;
        this.ltm = ltm;
        
        this.client = new OpenAI({ 
            apiKey: process.env.GEMINI_API_KEY, 
            baseURL: process.env.GEMINI_API_BASE_URL 
        });
        this.isBusy = false;
    }

    /**
     * æ ¸å¿ƒè™•ç†å…¥å£ (Process Input)
     * æ¥æ”¶ä½¿ç”¨è€…è¼¸å…¥ï¼ŒåŸ·è¡Œæ„ŸçŸ¥ã€æ±ºç­–ä¸¦å›å‚³å›æ‡‰ã€‚
     * @param {Object} payload - { conversationId, userText, attachments, mode, channelId }
     */
    async processInput(payload) {
        if (this.isBusy) {
            return { channelId: payload.channelId, messages: ["(æ€è€ƒä¸­...è«‹ç¨å€™)"], emotion: {} };
        }
        this.isBusy = true;

        try {
            const { conversationId, userText, attachments = [], mode = 'demon', channelId } = payload;
            
            // ğŸŒŸ æå– requestId (é€šå¸¸ç”±ä¸Šå±¤é€é channelId å‚³éä¸‹ä¾†)
            const requestId = channelId;

            // 1. é™„ä»¶å‰è™•ç† (åˆ†é›¢åœ–ç‰‡èˆ‡æ–‡å­—)
            const { imageParts, textContent } = this._processAttachments(userText, attachments);
            const safeText = textContent || (imageParts.length > 0 ? "(User uploaded an image)" : "(Empty)");

            // 2. æ„ŸçŸ¥èˆ‡è¨˜æ†¶å›æ†¶ (Perception & Recall)
            const [moodState, memoryContext] = await Promise.all([
                this.emotion.perceive(conversationId, safeText, mode),
                this.persona.recall(conversationId)
            ]);

            const ragMemories = ""; 

            const context = { moodState, memoryContext, ragMemories };
            const history = await this._loadHistoryAsMessages(conversationId);

            let finalOutputMessages = [];

            // 3. äººæ ¼è·¯ç”± (Persona Routing)
            if (mode === 'group') {
                appLogger.info('[Cognition] Entering Group Director Mode...');
                
                // å‘¼å«ç¾¤çµ„ç·¨æ’å™¨ (æ”¯æ´å·¥å…·èˆ‡è¦–è¦º)
                const responseChain = await groupChatService.orchestrateConversation(
                    safeText, 
                    context, 
                    this.ltm, 
                    history, 
                    imageParts
                );
                
                // æ ¼å¼åŒ–è¼¸å‡º
                finalOutputMessages = responseChain.map(item => 
                    `[SPEAKER:${item.speaker}]${item.content}`
                );
                
            } else if (mode === 'angel') {
                // ğŸŒŸ å‚³é requestId çµ¦æ€è€ƒè¿´åœˆ
                const angelReply = await this._runPrimaryPersona('angel', textContent, imageParts, history, context, requestId);
                finalOutputMessages = [angelReply]; 
            } else {
                // ğŸŒŸ å‚³é requestId çµ¦æ€è€ƒè¿´åœˆ
                const demonReply = await this._runPrimaryPersona('demon', textContent, imageParts, history, context, requestId);
                finalOutputMessages = [demonReply];
            }

            // 4. è¨˜æ†¶å­˜æª” (Memory Storage)
            const fullLog = finalOutputMessages.join('\n');
            await this._saveHistory(conversationId, safeText, fullLog, { mode });
            
            // å¯«å…¥ LTM (éåŒæ­¥åŸ·è¡Œï¼Œä¸é˜»å¡å›æ‡‰)
            this.persona.memorize(conversationId, safeText, fullLog, mode).catch(err => {
                appLogger.warn('[Cognition] LTM Memorize failed:', err);
            });

            // 5. å›å‚³çµæœ
            return {
                channelId: payload.channelId,
                messages: finalOutputMessages,
                mode: mode,
                emotion: {
                    demon_mood: moodState.values.demon_mood,
                    demon_affection: moodState.values.demon_affection,
                    demon_trust: moodState.values.demon_trust,
                    angel_mood: moodState.values.angel_mood,
                    angel_affection: moodState.values.angel_affection,
                    angel_trust: moodState.values.angel_trust
                }
            };

        } catch (e) {
            appLogger.error('[Cognition] Process Error:', e);
            return { channelId: payload.channelId, messages: ["(ç³»çµ±æ ¸å¿ƒé‹ç®—éŒ¯èª¤)"], emotion: {} };
        } finally {
            this.isBusy = false;
        }
    }

    /**
     * é™„ä»¶è™•ç†å™¨
     * å°‡ä¸Šå‚³æª”æ¡ˆè½‰æ›ç‚º LLM å¯ç†è§£çš„æ ¼å¼ (Vision / Text)
     */
    _processAttachments(originalText, attachments) {
        let finalText = originalText || "";
        const imageParts = [];

        if (!attachments || attachments.length === 0) {
            return { imageParts, textContent: finalText };
        }

        for (const file of attachments) {
            if (file.mimeType.startsWith('image/')) {
                imageParts.push({
                    type: "image_url",
                    image_url: { url: `data:${file.mimeType};base64,${file.data}` }
                });
            } else if (this._isTextFile(file)) {
                try {
                    const decodedText = Buffer.from(file.data, 'base64').toString('utf-8');
                    finalText += `\n\n--- [File: ${file.name}] ---\n${decodedText}\n--- [End of File] ---`;
                } catch (e) {
                    appLogger.warn(`[Cognition] Failed to decode text file: ${file.name}`);
                }
            }
        }
        
        return { imageParts, textContent: finalText };
    }

    _isTextFile(file) {
        return file.mimeType.startsWith('text/') || 
               file.mimeType.includes('json') || 
               file.mimeType.includes('javascript') ||
               /\.(js|py|md|txt|html|css|json)$/.test(file.name);
    }

    /**
     * åŸ·è¡Œå–®ä¸€äººæ ¼ (Angel/Demon)
     */
    async _runPrimaryPersona(type, userText, imageParts, history, context, requestId) {
        const coreSystemPrompt = type === 'angel' 
            ? getAngelSystemPrompt(context) 
            : getDemonSystemPrompt(context);

        const fullSystemPrompt = [
            coreSystemPrompt,
            getNaturalConversationInstruction(),
            getInteractionRulesPrompt()
        ].join('\n\n');
        
        const userContentPayload = (imageParts && imageParts.length > 0)
            ? [{ type: "text", text: userText || "è«‹åˆ†æé€™å¼µåœ–ç‰‡ã€‚" }, ...imageParts]
            : userText;

        const messages = [
            { role: 'system', content: fullSystemPrompt },
            ...history,
            { role: 'user', content: userContentPayload }
        ];

        return await this._executeLLM(messages, 0, requestId);
    }

    /**
     * LLM åŸ·è¡Œè¿´åœˆ (æ”¯æ´å·¥å…·èª¿ç”¨èˆ‡éè¿´æ€è€ƒ)
     */
    async _executeLLM(messages, depth = 0, requestId = null) {
        if (depth > MAX_THOUGHT_DEPTH) return "(æ€è€ƒè¿´åœˆéæ·±ï¼Œå¼·åˆ¶ä¸­æ–·)";
        
        const res = await this.client.chat.completions.create({
            model: MODEL_NAME, 
            messages, 
            tools: toolsDeclarations, 
            tool_choice: 'auto'
        });

        const msg = res.choices[0].message;
        
        // å¦‚æœ LLM æ±ºå®šå‘¼å«å·¥å…·
        if (msg.tool_calls) {
            // ğŸŒŸ ç™¼é€å¿ƒè·³åŒ…ï¼šé€šçŸ¥ Server "æˆ‘é‚„æ´»è‘—ï¼Œæ­£åœ¨ä½¿ç”¨å·¥å…·ï¼Œä¸è¦åˆ‡æ–·é€£ç·šï¼"
            if (parentPort && requestId) {
                appLogger.info(`[Cognition] ğŸ’— ç™¼é€å¿ƒè·³åŒ…å»¶é•·è¶…æ™‚ç­‰å¾… (Request: ${requestId})`);
                parentPort.postMessage({ type: 'WEB_CHAT_HEARTBEAT', requestId });
            }

            const nextMsgs = [...messages, msg];
            
            for (const call of msg.tool_calls) {
                try {
                    const args = JSON.parse(call.function.arguments);
                    const output = await executeTool(call.function.name, args);
                    
                    this.ltm.record({ 
                        type: 'tool_use', 
                        action: call.function.name, 
                        trigger: JSON.stringify(args),
                        result: String(output).slice(0, 200)
                    });
                    
                    nextMsgs.push({ 
                        role: 'tool', 
                        tool_call_id: call.id, 
                        name: call.function.name, 
                        content: String(output) 
                    });
                } catch (toolErr) {
                    nextMsgs.push({ 
                        role: 'tool', 
                        tool_call_id: call.id, 
                        name: call.function.name, 
                        content: `Error: ${toolErr.message}` 
                    });
                }
            }
            // éè¿´èª¿ç”¨ï¼Œè®“ LLM æ ¹æ“šå·¥å…·çµæœç”Ÿæˆå›æ‡‰
            return await this._executeLLM(nextMsgs, depth + 1, requestId);
        }
        
        return msg.content || "...";
    }

    // ============================================================
    // å…§éƒ¨äº‹ä»¶è™•ç† (Internal Impulses)
    // ============================================================

    async handleInternalImpulse(impulse) {
        if (this.isBusy) return null;
        
        if (impulse.type === 'TRIGGER_SELF_REFLECTION') {
            await this._performSelfReflection();
            return null;
        }

        if (impulse.type !== 'IDLE_CHECK') return null;

        const conversationId = await this._getMostActiveUser();
        if (!conversationId) return null;

        const state = await this.emotion.getState(conversationId);
        
        let lastActivityStr = state.values.last_user_activity || state.values.last_interaction_at;

        if (!lastActivityStr) {
            appLogger.warn(`[Cognition] ç”¨æˆ¶ ${conversationId} ç„¡æ´»å‹•ç´€éŒ„ï¼ŒåŸ·è¡Œè‡ªå‹•ä¿®å¾© (Touch Timer)...`);
            await this.repo.updateUserActivity(conversationId);
            return null;
        }

        const lastActiveTime = new Date(lastActivityStr).getTime();
        if (isNaN(lastActiveTime)) {
            appLogger.warn(`[Cognition] æ™‚é–“æ ¼å¼éŒ¯èª¤ (${lastActivityStr})ï¼Œå¼·åˆ¶é‡ç½®...`);
            await this.repo.updateUserActivity(conversationId);
            return null;
        }

        const now = Date.now();
        const idleTimeMinutes = (now - lastActiveTime) / (1000 * 60);

        appLogger.info(`[Cognition] Idle Check: ${idleTimeMinutes.toFixed(1)}m | User: ${conversationId}`);

        if (idleTimeMinutes > 60) {
            if (Math.random() > 0.7) {
                return await this._runBackgroundChat(conversationId, state);
            }
        }
        
        return null;
    }

    async _runBackgroundChat(conversationId, state) {
        appLogger.info('[Cognition] å•Ÿå‹•é–’ç½®å°åŠ‡å ´...');
        
        const context = {
            moodState: state,
            memoryContext: { factsText: "ç„¡" }, 
            ragMemories: ""
        };

        try {
            const responseChain = await groupChatService.runIdleChat(context, this.ltm);
            
            if (!responseChain || responseChain.length === 0) return null;

            const formattedMessages = responseChain.map(item => 
                `[SPEAKER:${item.speaker}]${item.content}`
            );

            const fullLog = formattedMessages.join('\n');
            await this._saveHistory(conversationId, "(System: Idle Trigger)", fullLog, { mode: 'group' });

            return { 
                channelId: conversationId,
                messages: formattedMessages,
                emotion: state.values,
                mode: 'group'
            };

        } catch (e) {
            appLogger.error('[Cognition] Idle chat failed:', e);
            return null;
        }
    }

    async _performSelfReflection() {
        appLogger.info('[Cognition] åˆå¤œåæ€å•Ÿå‹•...');
        try {
            const recentMemories = await this.ltm.retrieve({ period: '24h', limit: 10 });
            
            if (!recentMemories || recentMemories.length === 0) {
                appLogger.info('[Cognition] ä»Šæ—¥ç„¡æ–°è¨˜æ†¶ï¼Œè·³éåæ€ã€‚');
                return;
            }
            
            const reflectionPrompt = getSelfReflectionPrompt(recentMemories);
            
            const response = await this.client.chat.completions.create({
                model: MODEL_NAME, 
                messages: [{ role: 'user', content: reflectionPrompt }], 
                response_format: { type: "json_object" }
            });
            
            const content = response.choices[0].message.content;
            if (!content) return;

            const result = JSON.parse(content);
            
            if (result.insights && Array.isArray(result.insights)) {
                for (const insight of result.insights) {
                    await this.ltm.addReflection(insight.memory_id, insight.reflection_text);
                }
                appLogger.info(`[Cognition] åæ€å®Œæˆï¼Œç”Ÿæˆäº† ${result.insights.length} æ¢æ´è¦‹ã€‚`);
            }
            
        } catch(e) { 
            appLogger.error('[Cognition] åæ€å¤±æ•—:', e); 
        }
    }

    // ============================================================
    // Repository Access Wrappers
    // ============================================================

    async _loadHistoryAsMessages(id) {
        const history = await this.repo.getHistory(id);
        return history.slice(-MAX_HISTORY_CONTEXT);
    }
    
    async _saveHistory(id, u, a, meta = {}) { 
        const hist = await this.repo.getHistory(id);
        const mode = meta.mode || 'demon';
        
        const userMsg = { role: 'user', content: u, timestamp: new Date().toISOString(), meta: { target: mode } };
        const assistantMsg = { role: 'assistant', content: a, timestamp: new Date().toISOString(), meta: { speaker: mode } };

        const newHist = [...hist, userMsg, assistantMsg].slice(-MAX_HISTORY_STORE);
        
        await this.repo.saveHistory(id, newHist);
    }

    async _getMostActiveUser() {
        return await this.repo.getMostActiveUser();
    }
}